{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold\">Training - Pipeline</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Notebook Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtxsouza/.cache/pypoetry/virtualenvs/chatbot-41bQ2a1z-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# system\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import sys\n",
    "sys.path[0] = sys.path[0].replace('notebooks', '')\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "# model\n",
    "from model import ChatBot\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# data manipulation\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;33mENVIRONMENT\u001b[0m]: Local Machine\n",
      "Dataset located in /home/mtxsouza/projects/ChatBot/data/processed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/gdrive')\n",
    "    df_path = Path('/content/gdrive/MyDrive/')\n",
    "    figure_path = Path('/content')\n",
    "    weight_path = Path('/content/gdrive/MyDrive/')\n",
    "    ENV = 'Google Colab'\n",
    "except Exception as error:\n",
    "    df_path = Path(os.getcwd().replace('notebooks', 'data/processed'))\n",
    "    figure_path = Path(os.getcwd().replace('notebooks', 'figures'))\n",
    "    weight_path = Path(os.getcwd().replace('notebooks', 'model')).joinpath('weights')\n",
    "    weight_path.mkdir(exist_ok=True)\n",
    "    ENV = 'Local Machine'\n",
    "\n",
    "print(f'[\\033[1;33mENVIRONMENT\\033[0m]: {ENV}')\n",
    "print(f'Dataset located in {df_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Device (GPU|CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;36mGPU\u001b[0m]: \u001b[1;32mAvailable\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    gpu_available = '\\033[1;32mAvailable\\033[0m'\n",
    "else:\n",
    "    gpu_available = '\\033[1;31mNot Available\\033[0m'\n",
    "print(f'[\\033[1;36mGPU\\033[0m]: {gpu_available}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Custom Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(df_path.joinpath('cleaned_dataframe.csv'), encoding='utf-8', delimiter=\",\", on_bad_lines='skip')\n",
    "with open(file=df_path.joinpath('data_info.json'), mode='r') as json_file:\n",
    "    json_content = json.load(fp=json_file)\n",
    "    word_dict = json_content['word_dict']\n",
    "    question_len = json_content['question_length']\n",
    "    answer_len = json_content['answer_length']\n",
    "    vocabulary = json_content['num_vocabulary']\n",
    "    word_dict_num = {v: k for k, v in word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first work generally recognized artificial int...</td>\n",
       "      <td>warren mcculloch and walter pitts 1943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sources drawn formation first work generally r...</td>\n",
       "      <td>knowledge of the basic physiology and function...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>created hebbian learning rule</td>\n",
       "      <td>donald hebb 1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>first neural network built</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first neural network called</td>\n",
       "      <td>the snarc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question   \n",
       "0  first work generally recognized artificial int...  \\\n",
       "1  sources drawn formation first work generally r...   \n",
       "2                      created hebbian learning rule   \n",
       "3                         first neural network built   \n",
       "4                        first neural network called   \n",
       "\n",
       "                                             answers  \n",
       "0             warren mcculloch and walter pitts 1943  \n",
       "1  knowledge of the basic physiology and function...  \n",
       "2                                   donald hebb 1949  \n",
       "3                                               1950  \n",
       "4                                          the snarc  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframe_np = np.asarray(a=data_frame)\n",
    "\n",
    "split_size = [0.8, 0.1, 0.1]\n",
    "train_split, test_split, valid_split = random_split(dataset=cleaned_dataframe_np, lengths=split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = cleaned_dataframe_np[train_split.indices]\n",
    "test_df = cleaned_dataframe_np[test_split.indices]\n",
    "valid_df = cleaned_dataframe_np[valid_split.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (3377, 2)\n",
      "test : (422, 2)\n",
      "valid: (422, 2)\n"
     ]
    }
   ],
   "source": [
    "for df_name, data in zip(['train', 'test', 'valid'], [train_df, test_df, valid_df]):\n",
    "    print(f'{df_name:5}: {data.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Creating custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset: np.ndarray, word_dict: dict, voc_size: int, max_len_in: int = 30, max_len_out: int = 500) -> None:\n",
    "        \n",
    "        self.x = dataset[:,0]\n",
    "        self.y = dataset[:,1]\n",
    "\n",
    "        self.wd = word_dict\n",
    "        self.vs = voc_size\n",
    "        self.mli = max_len_in\n",
    "        self.mlo = max_len_out\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        \n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        x, y = self.gen_data(x=x, y=y)\n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def gen_data(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "\n",
    "        x_data.append(self.wd['<start>'])\n",
    "        for word in x.split(' '):\n",
    "            x_data.append(self.wd[word])\n",
    "        x_data.append(self.wd['<stop>'])\n",
    "        while len(x_data) < self.mli:\n",
    "            x_data.append(self.wd['<pad>'])\n",
    "        x_data = np.asarray(a=x_data)\n",
    "\n",
    "        y_data.append(self.wd['<start>'])\n",
    "        for word in y.split(' '):\n",
    "            y_data.append(self.wd[word])\n",
    "        y_data.append(self.wd['<stop>'])\n",
    "        while len(y_data) < self.mlo:\n",
    "            y_data.append(self.wd['<pad>'])\n",
    "        y_data = np.asarray(a=y_data)\n",
    "\n",
    "        return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_df = CustomDataset(dataset=train_df, word_dict=word_dict, voc_size=vocabulary, max_len_in=question_len, max_len_out=answer_len)\n",
    "test_custom_df = CustomDataset(dataset=test_df, word_dict=word_dict, voc_size=vocabulary, max_len_in=question_len, max_len_out=answer_len)\n",
    "valid_custom_df = CustomDataset(dataset=valid_df, word_dict=word_dict, voc_size=vocabulary, max_len_in=question_len, max_len_out=answer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([30])\n",
      "Y: torch.Size([400])\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_custom_df:\n",
    "    print(f'X: {x.size()}')\n",
    "    print(f'Y: {y.size()}')\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatBot(in_f=question_len, out_shape=(vocabulary, answer_len), word_dict=word_dict, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Checking architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5249, 399])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build = model(\n",
    "    torch.zeros(size=(2,question_len)).to(device=device),\n",
    "    torch.zeros(size=(2,answer_len)).to(device=device)\n",
    ")\n",
    "build.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 - Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "early_stop = 5\n",
    "\n",
    "n_plots = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 - Initializing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_weights(module) -> None:\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatBot(\n",
       "  (l_h): Linear(in_features=30, out_features=512, bias=True)\n",
       "  (l_c): Linear(in_features=30, out_features=512, bias=True)\n",
       "  (emb): Embedding(5249, 399)\n",
       "  (lstm): LSTM(429, 512, batch_first=True)\n",
       "  (lin): Linear(in_features=512, out_features=5249, bias=True)\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(_init_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 - Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=word_dict['<pad>'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 - Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Training methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_loss(loss: list, val_loss: list, epoch: int) -> None:\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "    for data, label in [(loss, 'train'), (val_loss, 'valid')]:\n",
    "        try:\n",
    "            ax.plot(np.arange(stop=epoch, step=epoch / len(data)), data, label=label)\n",
    "        except:\n",
    "            ax.plot(np.arange(stop=epoch, step=epoch / (len(data) - 0.5)), data, label=label)\n",
    "    \n",
    "    ax.set_xticks(np.arange(start=1, stop=epoch + 1))\n",
    "    ax.set_title(label='Loss graph', fontdict={'size': 10})\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_custom_df, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_custom_df, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 - Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 | Batch: 16 | Learing Rate: 0.001 | Loss Function: CrossEntropyLoss | Optimizer: Adam\n",
      "Overtting: 0/5 | Best loss: inf - Last savement: No saved yet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 7.893790245056152:   1%|          | 2/212 [00:06<11:35,  3.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(yhat\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vocabulary), y[:,\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     39\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 40\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     41\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m \u001b[39m# saving train results\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-41bQ2a1z-py3.10/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/chatbot-41bQ2a1z-py3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_epoch = 'No saved yet'\n",
    "\n",
    "previous_train = float('inf')\n",
    "previous_valid = float('inf')\n",
    "\n",
    "valid_x = []\n",
    "valid_y = []\n",
    "valid_yhat = []\n",
    "\n",
    "overfitting = 0\n",
    "\n",
    "\n",
    "for curr_epoch in range(1, epochs+1):\n",
    "\n",
    "    print(f'Epoch: {curr_epoch}/{epochs} | Batch: {batch_size} | Learing Rate: {lr} | Loss Function: {loss_fn.__class__.__name__} | Optimizer: {optimizer.__class__.__name__}')\n",
    "    print(f'Overtting: {overfitting}/{early_stop} | Best loss: {best_loss} - Last savement: {best_epoch}')\n",
    "\n",
    "    if overfitting == early_stop:\n",
    "        print('\\033[1;31mOVERFITTING\\033[0m')\n",
    "        break\n",
    "\n",
    "    # training\n",
    "    train_iter = tqdm.tqdm(iterable=train_loader)\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for x, y in train_iter:\n",
    "\n",
    "        x = x.to(device=device)\n",
    "        y = y.to(device=device)\n",
    "\n",
    "        yhat = model(x, y.long())\n",
    "\n",
    "        loss = loss_fn(yhat.view(-1, vocabulary), y[:,1:].reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # saving train results\n",
    "        train_losses.append(loss.cpu().detach().numpy())\n",
    "        train_iter.set_description(desc=f'Loss: {loss.item()}')\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        del x, y, yhat, loss\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # validing\n",
    "    valid_x.clear()\n",
    "    valid_y.clear()\n",
    "    valid_yhat.clear()\n",
    "    valid_iter = tqdm.tqdm(iterable=valid_loader)\n",
    "    valid_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_iter:\n",
    "\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            yhat = model(x, y.long())\n",
    "\n",
    "            loss = loss_fn(yhat.view(-1, vocabulary), y[:,1:].reshape(-1))\n",
    "\n",
    "            # saving valid results\n",
    "            valid_losses.append(loss.cpu().detach().numpy())\n",
    "            valid_iter.set_description(desc=f'Valid loss: {loss.item()}')\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            # saving samples\n",
    "            if len(valid_x) < n_plots:\n",
    "                if np.random.random() > .7:\n",
    "\n",
    "                    rand_idx = np.random.choice(a=x.size(0), size=1).item()\n",
    "\n",
    "                    valid_x.append(x[rand_idx,:].unsqueeze(0).cpu())\n",
    "                    valid_y.append(y[rand_idx,:].unsqueeze(0).cpu())\n",
    "                    valid_yhat.append(yhat[rand_idx,:,:].unsqueeze(0).cpu().argmax(dim=1))\n",
    "            \n",
    "            del x, y, yhat, loss\n",
    "\n",
    "    valid_loss /= len(valid_loader)\n",
    "    \n",
    "    # saving model\n",
    "    if best_loss > valid_loss:\n",
    "        best_loss = valid_loss\n",
    "        best_epoch = curr_epoch\n",
    "        torch.save(obj=model.state_dict(), f=weight_path.joinpath('chatbot_weights.pt'))\n",
    "    \n",
    "    # checking overfits\n",
    "    if previous_train > train_loss and previous_valid < valid_loss:\n",
    "        overfitting += 1\n",
    "    else:\n",
    "        if overfitting > 0:\n",
    "            overfitting -= 1\n",
    "    previous_train = train_loss\n",
    "    previous_valid = valid_loss\n",
    "    \n",
    "    # plotting graphs\n",
    "    clear_output()\n",
    "    plot_train_loss(loss=train_losses, val_loss=valid_losses, epoch=curr_epoch)\n",
    "    for x, y, yhat in zip(valid_x, valid_y, valid_yhat):\n",
    "        for content, name in zip((x[0], y[0], yhat[0]), ('question', 'answer', 'predict')):\n",
    "            text = []\n",
    "            for values in content:\n",
    "                word = word_dict_num[values.item()].replace('<start>', '').replace('<stop>', '').replace('<pad>', '')\n",
    "                if len(word):\n",
    "                    text.append(word)\n",
    "            text = ' '.join(text).strip()\n",
    "            print(f'{name:10}: {text}')\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-J8Vz-3tK-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
