{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold\">Training - Pipeline</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Notebook Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "# data manipulation\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;33mENVIRONMENT\u001b[0m]: Local Machine\n",
      "Dataset located in c:\\Users\\Matheus\\WorkSpace\\ChatBot\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/gdrive')\n",
    "    df_path = Path('/content/gdrive/MyDrive/')\n",
    "    figure_path = Path('/content')\n",
    "    ENV = 'Google Colab'\n",
    "except Exception as error:\n",
    "    df_path = Path(os.getcwd().replace('notebooks', 'data/processed'))\n",
    "    figure_path = Path(os.getcwd().replace('notebooks', 'figures'))\n",
    "    ENV = 'Local Machine'\n",
    "\n",
    "print(f'[\\033[1;33mENVIRONMENT\\033[0m]: {ENV}')\n",
    "print(f'Dataset located in {df_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Device (GPU|CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[1;36mGPU\u001b[0m]: \u001b[1;31mNot Available\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda':\n",
    "    gpu_available = '\\033[1;32mAvailable\\033[0m'\n",
    "else:\n",
    "    gpu_available = '\\033[1;31mNot Available\\033[0m'\n",
    "print(f'[\\033[1;36mGPU\\033[0m]: {gpu_available}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Custom Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(df_path.joinpath('cleaned_dataframe.csv'), encoding='utf-8', delimiter=\",\", on_bad_lines='skip')\n",
    "with open(file=df_path.joinpath('data_info.json'), mode='r') as json_file:\n",
    "    json_content = json.load(fp=json_file)\n",
    "    word_dict = json_content['word_dict']\n",
    "    question_len = json_content['question_length']\n",
    "    answer_len = json_content['answer_length']\n",
    "    vocabulary = json_content['num_vocabulary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first work generally recognized artificial int...</td>\n",
       "      <td>warren mcculloch and walter pitts 1943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sources drawn formation first work generally r...</td>\n",
       "      <td>knowledge of the basic physiology and function...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>created hebbian learning rule</td>\n",
       "      <td>donald hebb 1949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>first neural network built</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first neural network called</td>\n",
       "      <td>the snarc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question   \n",
       "0  first work generally recognized artificial int...  \\\n",
       "1  sources drawn formation first work generally r...   \n",
       "2                      created hebbian learning rule   \n",
       "3                         first neural network built   \n",
       "4                        first neural network called   \n",
       "\n",
       "                                             answers  \n",
       "0             warren mcculloch and walter pitts 1943  \n",
       "1  knowledge of the basic physiology and function...  \n",
       "2                                   donald hebb 1949  \n",
       "3                                               1950  \n",
       "4                                          the snarc  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframe_np = np.asarray(a=data_frame)\n",
    "\n",
    "split_size = [0.8, 0.1, 0.1]\n",
    "train_split, test_split, valid_split = random_split(dataset=cleaned_dataframe_np, lengths=split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = cleaned_dataframe_np[train_split.indices]\n",
    "test_df = cleaned_dataframe_np[test_split.indices]\n",
    "valid_df = cleaned_dataframe_np[valid_split.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (3377, 2)\n",
      "test : (422, 2)\n",
      "valid: (422, 2)\n"
     ]
    }
   ],
   "source": [
    "for df_name, data in zip(['train', 'test', 'valid'], [train_df, test_df, valid_df]):\n",
    "    print(f'{df_name:5}: {data.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Creating custom dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset: np.ndarray, word_dict: dict, voc_size: int, max_len_in: int = 30, max_len_out: int = 500) -> None:\n",
    "        \n",
    "        self.x = dataset[:,0]\n",
    "        self.y = dataset[:,1]\n",
    "\n",
    "        self.wd = word_dict\n",
    "        self.vs = voc_size\n",
    "        self.mli = max_len_in\n",
    "        self.mlo = max_len_out\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        \n",
    "        x = self.x[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        x, y = self.gen_data(x=x, y=y)\n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def gen_data(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        x_data = []\n",
    "        y_data = np.zeros(shape=[self.vs, self.mlo])\n",
    "        y_data[self.wd['<pad>'],:] = 1\n",
    "\n",
    "        x_data.append(self.wd['<start>'])\n",
    "        for word in x.split(' '):\n",
    "            x_data.append(self.wd[word])\n",
    "        x_data.append(self.wd['<stop>'])\n",
    "        while len(x_data) < self.mli:\n",
    "            x_data.append(self.wd['<pad>'])\n",
    "        x_data = np.asarray(a=x_data)\n",
    "\n",
    "        for idx, word in enumerate(y.split(' ')):\n",
    "            num = self.wd[word]\n",
    "            y_data[num,idx] = 1\n",
    "            y_data[self.wd['<pad>'],idx] = 0\n",
    "        y_data[self.wd['<stop>'],idx + 1] = 1\n",
    "        y_data[self.wd['<pad>'],idx + 1] = 0\n",
    "\n",
    "        return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_custom_df = CustomDataset(dataset=train_df, word_dict=word_dict, voc_size=vocabulary, max_len_in=question_len, max_len_out=answer_len)\n",
    "test_custom_df = CustomDataset(dataset=test_df, word_dict=word_dict, voc_size=vocabulary, max_len_in=question_len, max_len_out=answer_len)\n",
    "valid_custom_df = CustomDataset(dataset=valid_df, word_dict=word_dict, voc_size=vocabulary, max_len_in=question_len, max_len_out=answer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([30])\n",
      "Y: torch.Size([5249, 400])\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_custom_df:\n",
    "    print(f'X: {x.size()}')\n",
    "    print(f'Y: {y.size()}')\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot(nn.Module):\n",
    "\n",
    "    def __init__(self, in_f: int, out_shape: tuple, word_dict: dict) -> None:\n",
    "        \n",
    "        self.len = out_shape[1]\n",
    "        self.w = out_shape[0]\n",
    "        self.lstm = nn.LSTM(input_size=in_f, hidden_size=1024, num_layers=2)\n",
    "        self.lin = nn.Linear(in_features=1024, out_features=out_shape[0])\n",
    "        self.emb = nn.Embedding(num_embeddings=len(word_dict), embedding_dim=self.len)\n",
    "    \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \n",
    "        pred = torch.zeros(size=(x.size(0), self.w, self.len)).to(device=device)\n",
    "\n",
    "        x = self.emb(x)\n",
    "        h, c = 0, 0\n",
    "\n",
    "        for idx in range(self.len):\n",
    "            out, (h, c) = self.lstm(x[:,idx], (h, c))\n",
    "            out = self.lin(out)\n",
    "            pred[:,idx] = out\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def gen_text(self, x) -> torch.Tensor:\n",
    "        \n",
    "        pred = torch.zeros(size=(x.size(0), self.w, self.len)).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-J8Vz-3tK-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
